{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cktran_09859713.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu"
      },
      "source": [
        "#EECS 442 PS4: Backpropagation\n",
        "\n",
        "Calvin Tran, cktran\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHumIO-xt57H"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98aoizX_UG-W"
      },
      "source": [
        "# Problem 4.1 Understanding Backpropagation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IyQj_N-UPC-"
      },
      "source": [
        "# 4.1 (b)  \n",
        "Implement the code for forward and backward pass of computation graph in (a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIOVvqcPUf1-"
      },
      "source": [
        "def f_1(x0, x1, w0, w1, w2):\n",
        "    \"\"\"\n",
        "    Computes the forward and backward pass through the computational graph \n",
        "    of (a)\n",
        "\n",
        "    Inputs:\n",
        "    - x0, x1, w0, w1, w2: Python floats\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - L: The output of the graph\n",
        "    - grads: A tuple (grad_x0, grad_x1, grad_w0, grad_w1, grad_w2)\n",
        "    giving the derivative of the output L with respect to each input.\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass for the computational graph for (a) and#\n",
        "    # store the output of this graph as L                                     #\n",
        "    ###########################################################################\n",
        "    a = w0 * x0\n",
        "    b = w1 * x1\n",
        "    p = a + b\n",
        "    c = p + w2\n",
        "    n = c * (-1)\n",
        "    e = exp(n)\n",
        "    d = e + 1\n",
        "    L = 1/d\n",
        "    ###########################################################################\n",
        "    #                              END OF YOUR CODE                           #\n",
        "    ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the backward pass for the computational graph for (a)   #\n",
        "    # Store the gradients for each input                                      #\n",
        "    ###########################################################################\n",
        "    grad_L = 1\n",
        "    grad_d = -1/d**2\n",
        "    grad_e = grad_L\n",
        "    grad_n = e\n",
        "    grad_c = -1\n",
        "    grad_p = grad_L\n",
        "    grad_a = grad_L\n",
        "    grad_b = grad_L\n",
        "    \n",
        "    grad_x0 = grad_d * grad_e * grad_n * grad_c * grad_p * grad_a * w_0\n",
        "    grad_x1 = grad_d * grad_e * grad_n * grad_c * grad_p * grad_b * w_1\n",
        "    grad_w0 = grad_d * grad_e * grad_n * grad_c * grad_p * grad_a * x_0\n",
        "    grad_w1 = grad_d * grad_e * grad_n * grad_c * grad_p * grad_b * x_1\n",
        "    grad_w2 = grad_d * grad_e * grad_n * grad_c * grad_L\n",
        "    ###########################################################################\n",
        "    #                              END OF YOUR CODE                           #\n",
        "    ###########################################################################\n",
        "\n",
        "    grads = (grad_x0, grad_x1, grad_w0, grad_w1, grad_w2)\n",
        "    return L, grads"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErRvlYzAYrTm"
      },
      "source": [
        "# 4.1 (c)  \n",
        "Implement the code for forward and backward pass of computation graph in (c)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmh5UlxIY0re"
      },
      "source": [
        "def f_2(x, y, z):\n",
        "    \"\"\"\n",
        "    Computes the forward and backward pass through the computational graph \n",
        "    of (c)\n",
        "\n",
        "    Inputs:\n",
        "    - x, y, z: Python floats\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - L: The output of the graph\n",
        "    - grads: A tuple (grad_x, grad_y, grad_z)\n",
        "    giving the derivative of the output L with respect to each input.\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass for the computational graph for (c) and#\n",
        "    # store the output of this graph as L                                     #\n",
        "    ###########################################################################\n",
        "    a = x * (-1)\n",
        "    b = exp(y)\n",
        "    b1 = b\n",
        "    b2 = b\n",
        "    c = exp(z)\n",
        "    c1 = c\n",
        "    c2 = c\n",
        "    p = b1 * c1\n",
        "    p1 = p\n",
        "    p2 = p\n",
        "    d = b2 + p1\n",
        "    e = c2 / p2\n",
        "    m = a - d\n",
        "    n = m / e\n",
        "    L = n**2\n",
        "    ###########################################################################\n",
        "    #                              END OF YOUR CODE                           #\n",
        "    ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the backward pass for the computational graph for (c)   #\n",
        "    # Store the gradients for each input                                      #\n",
        "    ###########################################################################\n",
        "    grad_L = 1\n",
        "    grad_n = 2 * n\n",
        "    grad_m = grad_L / e\n",
        "    grad_e = -n / e\n",
        "    grad_d = -1\n",
        "\n",
        "    grad_p1 = grad_L\n",
        "    grad_p2 = -e / p2\n",
        "\n",
        "    grad_a = grad_L\n",
        "    \n",
        "    grad_b1 = grad_L * c1\n",
        "    grad_b2 = grad_L\n",
        "\n",
        "    grad_c1 = grad_L * b1\n",
        "\n",
        "    grad_x = grad_n * grad_m * grad_a * -1\n",
        "    grad_y = grad_n * ((grad_m * grad_d)(grad_p1 * grad_b1 * y + grad_b2 * y) + (grad_e * grad_p2 * grad_b1 * y))\n",
        "    grad_z = grad_n * grad_m * grad_d * grad_p1 * grad_c1 * z\n",
        "    ###########################################################################\n",
        "    #                              END OF YOUR CODE                           #\n",
        "    ###########################################################################\n",
        "\n",
        "    grads = (grad_x, grad_y, grad_z)\n",
        "    return L, grads"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC"
      },
      "source": [
        "# Problem 4.2 Softmax Classifier with Two Layer Neural Network\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu. Softmax layer has already been implemented in the provided code. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ljfgMv9PHx"
      },
      "source": [
        "def fc_forward(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input X has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - X: A numpy array containing input data, of shape (N, Din)\n",
        "    - W: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (X, W, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    \n",
        "    out = X.dot(W) + b\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (X, W, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - X: Input data, of shape (N, Din)\n",
        "      - W: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dX: Gradient with respect to X, of shape (N, Din)\n",
        "    - dW: Gradient with respect to W, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    X, W, b = cache\n",
        "    dX, dW, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    \n",
        "    dX = dout.dot(W.T)\n",
        "    dW = X.T.dot(dout)\n",
        "    db = np.sum(dout, axis=0)\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dX, dW, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x.copy()\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    \n",
        "    out = np.maximum(0, out)\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout.copy(), cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    \n",
        "    dx[x <= 0] = 0\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(X, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - X: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for X[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dX: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dX = None, None                                         #\n",
        "    dX = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    dX /= np.sum(dX, axis=1, keepdims=True)\n",
        "    loss = -np.sum(np.log(dX[np.arange(X.shape[0]), y])) / X.shape[0]\n",
        "    dX[np.arange(X.shape[0]), y] -= 1\n",
        "    dX /= X.shape[0]\n",
        "\n",
        "    return loss, dX"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz"
      },
      "source": [
        "# 4.2 (b) Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvxbx9UpxVL"
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        \n",
        "        self.params['W1'] = np.random.normal(scale=weight_scale, size=(input_dim, hidden_dim))\n",
        "        self.params['b1'] = np.zeros(hidden_dim)\n",
        "        self.params['W2'] = np.random.normal(scale=weight_scale, size=(hidden_dim, num_classes))\n",
        "        self.params['b2'] = np.zeros(num_classes)\n",
        "        \n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        \n",
        "        out_1, cache_1 = fc_forward(X, self.params['W1'], self.params['b1'])\n",
        "        relu_out, relu_cache = relu_forward(out_1) \n",
        "        out_2, cache_2 = fc_forward(relu_out, self.params['W2'], self.params['b2'])\n",
        "        scores = out_2\n",
        "        \n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "        \n",
        "        loss, dscores = softmax_loss(scores, y)\n",
        "        \n",
        "        dx_2, grads['W2'], grads['b2'] = fc_backward(dscores, cache_2)\n",
        "        dhidden = relu_backward(dx_2, relu_cache)\n",
        "        dx_1, grads['W1'], grads['b1'] = fc_backward(dhidden, cache_1)\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1a3d2c-5801-4272-a795-c4a218181ff0"
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def testNetwork(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "def SGD(W,dW, learning_rate=1e-3):\n",
        "    \"\"\" Apply a gradient descent step on weight W \n",
        "    Inputs:\n",
        "        W : Weight matrix\n",
        "        dW : gradient of weight, same shape as W\n",
        "        learning_rate : Learning rate. Defaults to 1e-3.\n",
        "    Returns:\n",
        "        new_W: Updated weight matrix\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n",
        "    new_W = W - learning_rate * dW\n",
        "\n",
        "    return new_W\n",
        "\n",
        "def trainNetwork(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    - optimizer: Choice of using either 'SGD' or 'SGD_Momentum' for updating weights; default is SGD.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    optimizer = kwargs.pop('optimizer', 'SGD')\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "    #Initialize velocity dictionary if optimizer is SGD_Momentum\n",
        "    if optimizer == 'SGD_Momentum':\n",
        "      velocity_dict = {p:np.zeros(w.shape) for p,w in model.params.items()}\n",
        "      \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        if optimizer == 'SGD':\n",
        "          for p, w in model.params.items():\n",
        "              model.params[p] = SGD(w,grads[p], learning_rate=learning_rate)\n",
        "\n",
        "        elif optimizer == 'SGD_Momentum':\n",
        "          for p, w in model.params.items():\n",
        "              model.params[p], velocity_dict[p] = SGD_Momentum(w, grads[p], velocity_dict[p], beta=0.5, learning_rate=learning_rate)\n",
        "        else:\n",
        "          raise NotImplementedError\n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = testNetwork(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = testNetwork(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters for SGD                          #\n",
        "#######################################################################\n",
        "\n",
        "# initialize model\n",
        "model_SGD = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n",
        "\n",
        "# start training using SGD\n",
        "model_SGD, train_acc_history_SGD, val_acc_history_SGD = trainNetwork(\n",
        "    model_SGD, train_data, learning_rate = 1e-2,\n",
        "    lr_decay=1, num_epochs=10, \n",
        "    batch_size=100, print_every=1000, optimizer = 'SGD')\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Iteration 1 / 4000) loss: 2.323817\n",
            "(Epoch 0 / 10) train acc: 0.098000; val_acc: 0.096400\n",
            "(Epoch 1 / 10) train acc: 0.409000; val_acc: 0.363900\n",
            "(Epoch 2 / 10) train acc: 0.440000; val_acc: 0.408800\n",
            "(Iteration 1001 / 4000) loss: 1.509000\n",
            "(Epoch 3 / 10) train acc: 0.459000; val_acc: 0.433100\n",
            "(Epoch 4 / 10) train acc: 0.451000; val_acc: 0.449000\n",
            "(Epoch 5 / 10) train acc: 0.494000; val_acc: 0.461500\n",
            "(Iteration 2001 / 4000) loss: 1.511527\n",
            "(Epoch 6 / 10) train acc: 0.538000; val_acc: 0.465700\n",
            "(Epoch 7 / 10) train acc: 0.516000; val_acc: 0.476500\n",
            "(Iteration 3001 / 4000) loss: 1.081032\n",
            "(Epoch 8 / 10) train acc: 0.528000; val_acc: 0.489900\n",
            "(Epoch 9 / 10) train acc: 0.546000; val_acc: 0.499300\n",
            "(Epoch 10 / 10) train acc: 0.604000; val_acc: 0.499200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ilTVXIw_7q"
      },
      "source": [
        "# 4.2(d) Training with SGD_Momentum\n",
        "\n",
        "The model above was trained using SGD. Now implement the SGD_Momentum function to train the model using SGD with momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54jGVPZOXtV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4a8aea-84ba-4759-dde4-8dd1c919feb5"
      },
      "source": [
        "def SGD_Momentum(W, dW, velocity, beta=0.5, learning_rate=1e-3):\n",
        "    \"\"\" Apply a gradient descent with momentum update on weight W\n",
        "    Inputs:\n",
        "        W : Weight matrix\n",
        "        dW : gradient of weight, same shape as W\n",
        "        velocity : velocity matrix, same shape as W\n",
        "        beta : scalar value in range [0,1] weighting the velocity matrix. Setting it to 0 should make SGD_Momentum same as SGD. \n",
        "               Defaults to 0.5.\n",
        "        learning_rate : Learning rate. Defaults to 1e-3.\n",
        "    Returns:\n",
        "        new_W: Updated weight matrix\n",
        "        new_velocity: Updated velocity matrix\n",
        "    \"\"\"\n",
        "    # ===== your code here! =====\n",
        "    # TODO:\n",
        "    # Apply a gradient descent step on weight W using the gradient dW and the specified learning rate.\n",
        "    # 1. Calculate the new velocity by using the velocity of last iteration (input velocity) and gradient\n",
        "    # 2. Update the weights using the new_velocity\n",
        "\n",
        "    new_velocity = dW + beta * velocity\n",
        "    new_W = W - learning_rate * velocity\n",
        "\n",
        "    # ==== end of code ====\n",
        "    return new_W, new_velocity\n",
        "\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters for SGD_Momentum   \n",
        "# Your hyperparameters should be identical to what you used for SGD (without momentum)#\n",
        "#######################################################################\n",
        "\n",
        "# initialize model\n",
        "model_SGD_Momentum = SoftmaxClassifier(hidden_dim = 300, weight_scale=1e-2)\n",
        "\n",
        "# start training \n",
        "#Using SGD_Momentum as optimizer for trainning for training\n",
        "model_SGD_Momentum, train_acc_history_SGD_Momentum, val_acc_history_SGD_Momentum = trainNetwork(\n",
        "    model_SGD_Momentum, train_data, learning_rate = 1e-2,\n",
        "    lr_decay=1, num_epochs=10, \n",
        "    batch_size=100, print_every=1000, optimizer = 'SGD_Momentum')\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Iteration 1 / 4000) loss: 2.292672\n",
            "(Epoch 0 / 10) train acc: 0.130000; val_acc: 0.120300\n",
            "(Epoch 1 / 10) train acc: 0.407000; val_acc: 0.398900\n",
            "(Epoch 2 / 10) train acc: 0.498000; val_acc: 0.445700\n",
            "(Iteration 1001 / 4000) loss: 1.598398\n",
            "(Epoch 3 / 10) train acc: 0.521000; val_acc: 0.462700\n",
            "(Epoch 4 / 10) train acc: 0.521000; val_acc: 0.477500\n",
            "(Epoch 5 / 10) train acc: 0.573000; val_acc: 0.494800\n",
            "(Iteration 2001 / 4000) loss: 1.180691\n",
            "(Epoch 6 / 10) train acc: 0.558000; val_acc: 0.499100\n",
            "(Epoch 7 / 10) train acc: 0.614000; val_acc: 0.502200\n",
            "(Iteration 3001 / 4000) loss: 1.298302\n",
            "(Epoch 8 / 10) train acc: 0.610000; val_acc: 0.507700\n",
            "(Epoch 9 / 10) train acc: 0.649000; val_acc: 0.512300\n",
            "(Epoch 10 / 10) train acc: 0.634000; val_acc: 0.507000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa"
      },
      "source": [
        "# 4.2(e) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy of model_SGD and model_SGD_Momentum on test set. Which model trains more quickly? Is the ultimate validation accuracy different? Report your observation in the text block below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCq8pBhu6dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f82bba-7c94-4ed0-8df6-b88fd0db5441"
      },
      "source": [
        "# report test accuracy\n",
        "acc = testNetwork(model_SGD, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy of model_SGD: {}\".format(acc))\n",
        "# report test accuracy\n",
        "acc = testNetwork(model_SGD_Momentum, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy of model_SGD_Momentum: {}\".format(acc))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy of model_SGD: 0.4983\n",
            "Test accuracy of model_SGD_Momentum: 0.506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQUQgfyCHzxn"
      },
      "source": [
        "My observation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N"
      },
      "source": [
        "# 4.2(f) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot, using SGD and SGD_Momentum as optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjtnbya9S7g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "e2d6fe6e-2ca5-48a2-bcf1-41a66313110e"
      },
      "source": [
        "#######################################################################\n",
        "# Your Code here\n",
        "#######################################################################\n",
        "f = plt.figure()\n",
        "f.set_figwidth(7)\n",
        "f.set_figheight(7)\n",
        "plt.title(\"Accuracies vs. Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.plot(train_acc_history_SGD_Momentum, label='Training Accuracy SGD Momentum')\n",
        "plt.legend()\n",
        "plt.plot(train_acc_history_SGD, label='Training Accuracy SGD')\n",
        "plt.legend()\n",
        "plt.plot(val_acc_history_SGD_Momentum, label='Validation Accuracy SGD Momentum')\n",
        "plt.legend()\n",
        "plt.plot(val_acc_history_SGD, label='Validation Accuracy SGD')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7MoINIIhYf7"
      },
      "source": [
        "# Convert Notebook to PDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24o3eOBHhZuR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce030191-3ada-4911-fd1e-74a3ad67d079"
      },
      "source": [
        "# generate pdf\n",
        "# %%capture\n",
        "!git clone https://gist.github.com/bc5f1add34fef7c7f9fb83d3783311e2.git\n",
        "!cp bc5f1add34fef7c7f9fb83d3783311e2/colab_pdf.py colab_pdf.py\n",
        "from colab_pdf import colab_pdf\n",
        "# change the name to your ipynb file name shown on the top left of Colab window\n",
        "# Important: make sure that your file name does not contain spaces!\n",
        "colab_pdf('cktran_09859713.ipynb')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bc5f1add34fef7c7f9fb83d3783311e2'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 0 (delta 0), pack-reused 10\u001b[K\n",
            "Unpacking objects: 100% (10/10), done.\n",
            "Mounted at /content/drive/\n",
            "Get:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,802 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [922 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,209 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,801 kB]\n",
            "Get:18 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.8 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,430 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,365 kB]\n",
            "Fetched 11.9 MB in 4s (3,302 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  javascript-common libcupsfilters1 libcupsimage2 libgs9 libgs9-common\n",
            "  libijs-0.35 libjbig2dec0 libjs-jquery libkpathsea6 libpotrace0 libptexenc1\n",
            "  libruby2.5 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13 lmodern\n",
            "  poppler-data preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-latex-base texlive-latex-extra\n",
            "  texlive-latex-recommended texlive-pictures texlive-plain-generic tipa\n",
            "Suggested packages:\n",
            "  fonts-noto apache2 | lighttpd | httpd poppler-utils ghostscript\n",
            "  fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic\n",
            "  | fonts-ipafont-gothic fonts-arphic-ukai fonts-arphic-uming fonts-nanum ri\n",
            "  ruby-dev bundler debhelper gv | postscript-viewer perl-tk xpdf-reader\n",
            "  | pdf-viewer texlive-fonts-recommended-doc texlive-latex-base-doc\n",
            "  python-pygments icc-profiles libfile-which-perl\n",
            "  libspreadsheet-parseexcel-perl texlive-latex-extra-doc\n",
            "  texlive-latex-recommended-doc texlive-pstricks dot2tex prerex ruby-tcltk\n",
            "  | libtcltk-ruby texlive-pictures-doc vprerex\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  javascript-common libcupsfilters1 libcupsimage2 libgs9 libgs9-common\n",
            "  libijs-0.35 libjbig2dec0 libjs-jquery libkpathsea6 libpotrace0 libptexenc1\n",
            "  libruby2.5 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13 lmodern\n",
            "  poppler-data preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-fonts-recommended texlive-generic-recommended\n",
            "  texlive-latex-base texlive-latex-extra texlive-latex-recommended\n",
            "  texlive-pictures texlive-plain-generic texlive-xetex tipa\n",
            "0 upgraded, 47 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 146 MB of archives.\n",
            "After this operation, 460 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lato all 2.0-2 [2,698 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 tex-common all 6.09 [33.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lmodern all 2.004.5-3 [4,551 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-texgyre all 20160520-1 [8,761 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 javascript-common all 11 [6,066 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.14 [5,092 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.14 [2,265 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjs-jquery all 3.2.1-1 [152 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkpathsea6 amd64 2017.20170613.44572-8ubuntu0.1 [54.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpotrace0 amd64 1.14-2 [17.4 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libptexenc1 amd64 2017.20170613.44572-8ubuntu0.1 [34.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 rubygems-integration all 1.11 [4,994 B]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ruby2.5 amd64 2.5.1-1ubuntu1.10 [48.6 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby amd64 1:2.5.1 [5,712 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 rake all 12.3.1-1ubuntu0.1 [44.9 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-did-you-mean all 1.2.0-2 [9,700 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-minitest all 5.10.3-1 [38.6 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-power-assert all 0.3.0-1 [7,952 B]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-test-unit all 3.2.5-1 [61.1 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libruby2.5 amd64 2.5.1-1ubuntu1.10 [3,071 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsynctex1 amd64 2017.20170613.44572-8ubuntu0.1 [41.4 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexlua52 amd64 2017.20170613.44572-8ubuntu0.1 [91.2 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexluajit2 amd64 2017.20170613.44572-8ubuntu0.1 [230 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzzip-0-13 amd64 0.13.62-3.1ubuntu0.18.04.1 [26.0 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/main amd64 lmodern all 2.004.5-3 [9,631 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 preview-latex-style all 11.91-1ubuntu1 [185 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/main amd64 t1utils amd64 1.41-2 [56.0 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tex-gyre all 20160520-1 [4,998 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 texlive-binaries amd64 2017.20170613.44572-8ubuntu0.1 [8,179 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-base all 2017.20180305-1 [18.7 MB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-recommended all 2017.20180305-1 [5,262 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-plain-generic all 2017.20180305-2 [23.6 MB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-generic-recommended all 2017.20180305-1 [15.9 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-base all 2017.20180305-1 [951 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-recommended all 2017.20180305-1 [14.9 MB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-pictures all 2017.20180305-1 [4,026 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-latex-extra all 2017.20180305-2 [10.6 MB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tipa all 2:1.3-20 [2,978 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-xetex all 2017.20180305-1 [10.7 MB]\n",
            "Fetched 146 MB in 3s (58.2 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../04-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../05-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../06-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../07-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../08-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../09-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../10-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../11-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../12-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.14_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../13-libgs9_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../14-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../15-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../16-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../17-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../18-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../19-ruby2.5_2.5.1-1ubuntu1.10_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.10) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../20-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../21-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../22-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../23-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../24-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../25-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../26-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../27-libruby2.5_2.5.1-1ubuntu1.10_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.10) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../28-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../29-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../30-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../31-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../32-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../33-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../34-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../35-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../36-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../37-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../38-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../39-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-generic-recommended.\n",
            "Preparing to unpack .../40-texlive-generic-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-generic-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../41-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../42-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../43-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../44-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../45-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Selecting previously unselected package texlive-xetex.\n",
            "Preparing to unpack .../46-texlive-xetex_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-xetex (2017.20180305-1) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up texlive-generic-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up texlive-xetex (2017.20180305-1) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.10) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.10) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "[NbConvertApp] Converting notebook /content/drive/My Drive/Colab Notebooks/cktran_09859713.ipynb to pdf\n",
            "[NbConvertApp] Support files will be in cktran_09859713_files/\n",
            "[NbConvertApp] Making directory ./cktran_09859713_files\n",
            "[NbConvertApp] Writing 132232 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 108881 bytes to /content/drive/My Drive/cktran_09859713.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_129b5a70-298c-47ed-9787-91fec8837a9d\", \"cktran_09859713.pdf\", 108881)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File ready to be Downloaded and Saved to Drive'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d22F8kh1JyE7"
      },
      "source": [
        "#Alternative way to convert pdf\n",
        "If the above method does not work, please look into [this instruction](https://docs.google.com/document/d/1QTutnoApRow8cOxNrKK6ISEkA72QGfwLFXbIcpvarAI/edit?usp=sharing)."
      ]
    }
  ]
}